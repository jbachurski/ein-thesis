\chapter{Implementation}

We now describe the implementation of the project. We consider the main parts of the compiler in sequence. We begin with the DSL front-end along with its underlying Phi calculus. Afterwards, we consider the compiler middle-end containing various analyses and transformations. Lastly, we describe the NumPy execution back-end, where we also provide a brief overview of a point-free array calculus.

% We describe the implementation from the front-end to the backend, describing the compilation pipeline:
% \begin{itemize}
%     \item Shallow-embedded domain-specific language --  \textit{Ein}
%     \item Pointful array calculus, the terms of which are constructed by the \textit{Ein} front-end -- \textit{Phi}
%     \item Pointless array calculus, constructed by way of compiling Phi -- \textit{Yarr}
%     \item Newly introduced applicative that bridges pointful and point-free array programming -- \textit{Axial}
%     \item Execution backends in NumPy and PyTorch
% \end{itemize}

% \section{Phi -- pointful calculus}
\section{Language -- Phi calculus}

We begin by introducing the \textbf{Phi calculus}, which underlies the implemented shallow-embedded DSL. The $\Phi$ array comprehension is its key feature. It is the basis for our embedded pointful array language. We outline the design choices made, and how they relate to the real capabilities of array libraries.

\subsection{Grammar and design}

\newcommand{\philet}[3]{\mathrm{let}\,{#1}={#2}\,\mathrm{in}\,{#3}}
\newcommand{\phivec}[3]{\Phi\, {#1}[{#2}] \ldotp {#3}}
\newcommand{\phifold}[5]{\mathrm{fold}\,{#1}[{#2}]\,\mathrm{over}\,{#3} = {#4}\,\mathrm{by}\,{#5}}
% \newcommand{\phiif}[3]{\mathrm{if}\,{#1} \,\mathrm{then}\, {#2}\, \mathrm{else}\, {#3}}
\newcommand{\phipair}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\phifst}[1]{\mathrm{fst}\,{#1}}
\newcommand{\phisnd}[1]{\mathrm{snd}\,{#1}}
\newcommand{\phisize}[2]{\mathrm{size}_{#2}\, {#1}}
\newcommand{\phiasserteq}[2]{\mathrm{assert}\,{#1}={#2}}

We define the syntax and primitives of Phi. It is largely similar to $\tilde F$, as introduced by \textcite{shaikhha2019efficient}.
\begin{align*}
e ::=&\quad \phivec{i}{e}{e} \quad|\quad e[e]   &\text{(array comprehension, indexing)} \\
|&\quad \phifold{x}{e}{x}{e}{e}  &\text{(indexed fold)} \\
|&\quad \phipair{e}{e} \quad|\quad \phifst{e} \quad|\quad \phisnd{e} &\text{(pair construction, projections)} \\
|&\quad \pi(e, \dots, e) &\text{(scalar operator)} \\
|&\quad \phiasserteq{e}{e} \quad|\quad \phisize{e}{k} &\text{(equality assertion, size along axis } k \in \mathbb N \text{)} \\
|&\quad \philet{x}{e}{e} &\text{(non-recursive let binding)} \\
|&\quad x \quad|\quad i \quad|\quad v &\text{(variable, index, constant)}
\end{align*}
The introduction form for arrays is the indexed \textit{array comprehension} $\Phi$ (pronounced \textit{for}) -- for instance, $\phivec{i}{5}{i}$ is the array $[0, 1, 2, 3, 4]$. On the other hand, the elimination form is \textit{indexing} $a[i]$ ($i$-th element of $a$). Phi interprets multidimensional arrays as either scalars (zero-dimensional base case) or vectors of arrays. In that respect, indexing is into the \textit{outermost} axis. 

The indexed fold facilitates a simple repeated iteration with an accumulator, and is closely related to the \texttt{loop} construct in Futhark. One can see $\Phi$ as perfectly parallel, while $\mathrm{fold}$ expresses sequential computation.

Examples of scalar operators $\pi$ include arithmetic ($+$, $\times$, \dots) and logic ($\land$, $\lor$, \dots) operators. Among non-standard primitives are equality assertions, which are applied to ensure that two array dimensions have equal sizes. These sizes are obtained with the $\mathrm{size}$ primitive -- if $e$ has shape $(n, m)$, then $\phisize{e}{1} = m$.

A crucial feature of Phi is the addition of a special kind of variable -- indices $i, j, k, \dots$ -- which live in a separate namespace. They are solely introduced in array comprehensions, and receive special treatment in both the type system and the compilation scheme. We use usual variables $x, y, z, \dots$ in all other cases.

For example, the following Phi term computes the (left-associative) sum $\sum_{i=0}^{n-1} a_i$ for a vector $a$:
$$ \phifold{i}{n}{x}{0.0}{x + a[i]} $$

\subsection{Type system}

\newcommand{\phifloattype}{\mathrm{Float}}
\newcommand{\phiinttype}{\mathrm{Int}}
\newcommand{\phinattype}{\phiinttype}
\newcommand{\phibooltype}{\mathrm{Bool}}
\newcommand{\phivectype}[1]{\Box{#1}}
\newcommand{\phipairtype}[2]{{#1} \times {#2}}

The type system of Phi is relatively straightforward, except for the handling of indices. Type constructors are unconstrained, and we allow e.g. arrays of pairs. 
\begin{align*}
\mathrm{\sigma} &::= \phifloattype \mid \phiinttype \mid \phibooltype & \text{(scalar types)} \\
\tau &::= \mathrm{\sigma} \mid \phivectype{\tau} \mid \phipairtype{\tau}{\tau} & \text{(Phi types -- scalars, vectors, pairs)}
\end{align*}
The typing judgement is slightly non-standard due to the presence of indices. We consider separate variable and index environments -- $\Gamma$ and $\Delta$, respectively. We then write $\Gamma; \Delta \vdash e : \tau$ for the judgement. We consider the judgements for array comprehensions and folds:
\begin{center}
    \begin{prooftree}
        \hypo{\Gamma; \diamond \vdash n : \phinattype}
        \hypo{\Gamma; \Delta, i \vdash e : \tau}
        \infer2{\Gamma; \Delta \vdash \phivec{i}{n}{e} : \phivectype{\tau}}
    \end{prooftree} \quad
    \begin{prooftree}
        \hypo{\Gamma; \diamond \vdash n : \phinattype}
        \hypo{\Gamma; \Delta \vdash a : \tau}
        \hypo{\Gamma, k: \phinattype, x: \tau; \Delta \vdash e : \tau}
        \infer3{\Gamma; \Delta \vdash \phifold{k}{n}{x}{a}{e} : \tau}
    \end{prooftree}
\end{center}
Note that sizes and iteration counts are typed under $\Delta = \diamond$ -- i.e., neither can depend on a comprehension index. This ensures \textit{regularity} of the parallelism involved. Since an array size cannot depend on the index at which the defined element is placed, all arrays must remain rectangular. The following does not type:
$$ \phivec{i}{5}{\phivec{j}{\textcolor{red}{i}}{i + j}} $$
Similarly, since all iteration counts are the same across all array elements, the same computation is applied at each index. This is a highly beneficial property that ensures the existence of an efficient compilation scheme. Furthermore, we achieve it by a simple type check that e.g. is glossed over in $\tilde F$, and is done by a runtime check in Futhark and a dependent type system in Dex.

Other typing rules are standard and carry through both $\Gamma$ and $\Delta$.

\subsection{Semantics}

\paragraph{Conditionals} The reader might notice the lack of a conditional expression. Indeed, in Phi we consider ternary conditionals to be a scalar operator. As such, both branches are always evaluated regardless of the condition. To make this clearer in notation we write conditionals as $\mathrm{where}(c, t, f)$. This design choice follows as the array programming model has no real notion of a \textit{branching} array computation -- both cases are evaluated, as this ensures efficient vectorisation (SIMD processing). In hardware, the related notion is \textit{predication} -- particularly in GPUs or in CPU conditional move instructions.

\paragraph{Out-of-bounds} As noted, ternary conditionals evaluate both of their branches. In particular, this means that indexing operations that take place in either branch might end up out of bounds (since we cannot guard them). 
The Jax framework tackles a similar problem, and in the context of hardware accelerators the best solution seems to be to gracefully recover from the error by clipping or wrapping the indices, or replacing results with a constant. 
In terms of the semantics of Phi we consider these to be an implementation detail. Instead, we consider indexing out of bounds to produce a \textit{poison} value, akin to integer overflows in LLVM. It should be noted Ein's backends clip indices to be in-bounds.

\paragraph{Invalid sizes} Where an array is defined with a negative size, or a fold was to perform a negative number of iterations, a runtime error is raised. 


% \section{Ein -- shallow-embedded language}
\section{Frontend -- Ein}

\textbf{Ein} forms the programmer-accessible side of the project, and is implemented as the \texttt{ein} Python package. Unlike a traditional compiler, our frontend is not formed by a lexer or parser, but as a Python library functioning as a shallow-embedded DSL. Terms in our underlying language, the Phi calculus, are constructed by way of operations on the programmatic interface of Ein. This section is an overview of the features of Ein. We also describe the metaprogramming techniques used to implement language features such as array size inference and record types.

\subsection{Design}

One of the main influences on the design is Jax \cite{frostig2018compiling}, which lays out the pattern of encapsulating expression types and building programs up by tracing. This indirection can be seen as an instance of multi-stage programming, since the constructed expression can have whole-program optimisations applied to it. A useful feature is that let bindings in the language are implicit, and instead object identity is used to establish the same expression is reused, which can be seen as a safe approximation of \textit{hash consing}. For instance where \texttt{x} is an Ein value, \texttt{x + x} indicates the reuse of the value \texttt{x} added to itself, rather than performing the same computation twice.

\subsection{Types and values}

Nearly all operations in Ein take place on values of the \texttt{Array} type. These incrementally construct expressions in the underlying calculus via tracing. For composability, Ein considers arrays to be defined recursively as either \textbf{scalars} or \textbf{vectors} of arrays. This corresponds to the \texttt{Scalar} and \texttt{Vec} classes. These classes form the actual implementation of the abstract \texttt{Array} class, considered as a sum type.

The methods of \texttt{Scalar} correspond to the scalar operators of Phi. These can be performed with operator overloading (as in \texttt{Scalar.\_\_mul\_\_} -- \texttt{a * b}) and member functions (\texttt{Scalar.sin} -- \texttt{a.sin()}). On the other hand, \texttt{Vec} implements \texttt{Vec.\_\_getitem\_\_}, so that we can write \texttt{a[i]}. These are all exemplified below:
\begin{center}
\begin{cminted}{python}
a: Vec[Scalar]
b: Scalar = a[0].sin() * a[1].cos()
\end{cminted}
\end{center}

All of the classes used for representing Ein values are usable as type annotations. For this purpose, we further split \texttt{Scalar} into \texttt{Int}, \texttt{Float} and \texttt{Bool}, which correspond to base types $\sigma$ in Phi. The annotation \mintinline{python}{a: Vec[Vec[Float]]} indicates \texttt{a} is an Ein matrix of floats. Thanks to this design, it is possible to use standard type checkers like \texttt{mypy} for \textbf{gradual typing} of Python programs. Thanks to this some errors -- like attempting to add a scalar and a vector, or indexing into a scalar -- may be discovered before runtime. This is a significant advancement over NumPy and its derivatives, where this sort of static type checking is nearly impossible.

Furthermore, during tracing Ein performs a form of basic type inference on the underlying Phi expressions. Any type errors found at that stage are reported at runtime, which ensures type safety of programs produced by the compiler. 

\subsection{Combinators}

The central part of Ein are the pointful, comprehension-style primitives: \texttt{array} and \texttt{fold}. Anonymous functions, defined with the \mintinline{python}{lambda} keyword, are applied to facilitate introduction of new variables -- for instance, the index in an array comprehension. Symbols for these variables are generated uniquely, and we compare them via Python object identity. As such, \mintinline{python}{array(lambda i: i, size=5)} describes the Phi expression $\phivec{i}{5}{i}$. Further, the summation in $\philet{a}{\phivec{i}{5}{i^2}}{ \sum_{i=0}^{4} a[i]}$ is expressed with a \texttt{fold}:
\begin{center}
\begin{cminted}{python}
a = array(lambda i: i*i, size=5)
s = fold(0, lambda i, acc: acc + a[i])
\end{cminted}
\end{center}
We avoid explicit variable introductions by making use of the host language's lambda functions, in a manner similar to \textcite{atkey2009unembedding} and Jax. The explicit approach is taken surprisingly often for its drawbacks -- for instance in the SymPy algebra library or TACO. In the latter this amounts to having to write:
\begin{center}
\begin{cminted}{python}
i, j = pytaco.get_index_vars(2)  # explicit introduction
S[i, j] = A[i, j] + A[j, i]      # i, j still live afterwards. danger!
\end{cminted}
\end{center}
This is evidently flawed, as there is nothing guarding against the reuse of variables in the wrong scope. 

It is worth noting that Ein's combinators introduce nested code blocks. This information is missing in traced expressions. Instead, we recover blocks via loop-invariant code motion in the middle-end.

Ein's combinators represent a step up in the expressive power of the array programming model. We previously noted NumPy is a first-order interface, in that its operations cannot be parameterised by functions. On the other hand, \texttt{array} and \texttt{fold} are second-order, similarly to Futhark's Second-Order Array Combinators. These are still beneficial despite no complete language support for higher-order functions.

\subsection{Size inference}

\textit{Size inference} is a form of syntactic sugar implemented in Ein, that can be seen as an instance of metaprogramming on the embedded DSL. It is a particularly common pattern, where a newly defined array shares the same size as the one that we are operating on. For example, consider the following computation:
\begin{center} 
\begin{cminted}{python}
array(lambda i: a[i] + b[i], size=a.size(axis=0))
\end{cminted} 
\end{center}
Say that the vectors $\texttt{a}$ and $\texttt{b}$ have the same size. Then with size inference we may omit the explicit \texttt{size}:
\begin{center} 
\begin{cminted}{python}
array(lambda i: a[i] + b[i])
\end{cminted} 
\end{center}
Specifically, for any index \texttt{i} that does not have an explicit \texttt{size} defined, it is inferred by taking the size of any array \texttt{a} that is indexed directly with \texttt{i} (i.e. in an expression \texttt{a[i]}), which we find by term graph traversal. Where there are other such candidates \texttt{b}, we add a program assertion that \texttt{a} and \texttt{b} have the same size. We further generalise this to the \texttt{fold} combinator. Its loop counter is often used to iterate over arrays.

This is not as obvious to implement correctly as it seems at first sight. Any sizes cannot depend on a comprehension index (due to the Phi typing rules) nor on a value from an inner scope. Various checks and simplifications are performed to avoid bad candidates for an inferred array size.

There are many mechanisms similar to size inference, for instance in Single-Assignment C and in a more structured way in Dex via its value-dependent type inference. In DSLs taking direct inspiration from Einstein summation, such approaches are often the only way of specifying array sizes. Ein also provides the flexibility of providing an explicit array size, as shown in the former example.

\subsection{Records}

One might notice the lack of inclusion of pairs in the \texttt{Array} type. These are modelled separately. In fact, pair types of Phi are sufficient to represent (labelled) record types. We use a basic list-like encoding where the type $\{ x: \phiinttype, y: \phiinttype, z: \phiinttype \}$ is represented as $\phipairtype{\phiinttype}{\left( \phiinttype \times \phiinttype \right)}$. For convenience, we directly use standard Python types to represent Ein record values. We can write:
\begin{center} 
\begin{cminted}{python}
a = array(lambda i: {"x": i, "y": i*i, "z": i*i*i}, size=10)
assert list(a[4].keys()) == ["x", "y", "z"]
assert a[4]["y"].eval() == 16
\end{cminted}
\end{center}
In fact, arbitrary \textit{layouts} consisting of Python tuples, dictionaries, and dataclasses can be used as array elements -- and these are reconstructed when indexing into these arrays. This feature relies on the layouts being \textit{static}. The only dynamically-sized values are arrays, which are handled through the \texttt{Vec} class. 

This feature enables a new style of array programming, unavailable in existing libraries. It is possible to describe composable array structures of custom data types and define operations on them. For instance, one could define an array of dual numbers\footnote{Dual numbers are similar to complex numbers, but instead of the imaginary unit $i^2 = -1$ we instead have a symbol $\varepsilon^2 = 0$. They are particularly useful in forward-mode automatic differentiation, and one could use them for this purpose in Ein.} with arrays of dataclasses:
\begin{center}
\begin{cminted}{python}
@dataclass
class Dual:
    real: Float
    eps: Float
    def __mul__(self, other: 'Dual') -> 'Dual':
        return Dual(
            self.real * other.real, 
            self.real * other.eps + self.eps * other.real
        )

a = array(lambda i: Dual(i, 1), size=5)  # constructs Vec[Dual]
b = array(lambda i: a[i] * a[i])  # calls Dual.__mul__
\end{cminted}
\end{center}
In contrast, traditional libraries in the array programming model struggle to achieve this sort of composability. Since they consider whole arrays at a time, at best one could define a series of types (vectors, matrices,~...) for dual numbers. In a pointful paradigm, where we focus on individual elements, we can define custom scalar datatypes and consider arrays of them. Futhark possesses similarly powerful ML-style record types.

What is more, efficient runtime representation of these structures is ensured via various program transformations. Record types in Ein are a \textbf{zero-cost abstraction}. We return to this claim in the Evaluation.

\section{Analyses}

We now describe the main analyses applied on Phi calculus, which facilitate further transformations and efficient code generation. Since Ein produces Phi term graphs through the tracing process, this is the main form on which we perform our analyses and transformations.

\subsection{Size equivalences}

\textit{Using equality assertions to create an equivalence judgement between sizes. Used for determining safety to elide some indexing operations and that operations do not explode memory usage.}
\todothis

\subsection{Specialisations}

\textit{Declarative syntax matching on summation, minimisation, etc. defined by folds, which allows us to infer what high-level operations are applied (e.g. reductions).} \todothis

\paragraph{Folds}

\paragraph{Clipped translations}

\paragraph{Tensor contractions} 

\section{Transformations}

\subsection{Outlining}

The term graph form produced by Ein is particularly useful when performing analyses, as not as much information has to be carried through in contexts. However, it is not an intermediate representation apt for generating executable code. It lacks information on where values used multiple times should be stored and when should they be computed. To address this, we want to convert our \textbf{term graph} into an \textbf{abstract syntax tree}. We refer to this process as \textit{outlining}. The goal is inserting appropriate let bindings in a semantic-preserving fashion, while preventing recomputation of expressions many times. Notably, \textit{inlining} (which is also used) corresponds to removal of the let bindings inserted by outlining. We perform it simply by directly substituting all bindings, returning the AST to the term graph form.

The two main techniques applied are \textbf{common subexpression elimination} and \textbf{loop-invariant code motion}. The former addresses avoiding computations like $e + e$ (for a \textit{non-atomic} expression $e$), which is replaced by $\philet{x}{e}{x + x}$. Meanwhile, the latter ensures that terms constant across loop iterations are computed outside the loop. For instance, we want to perform the following:
$$ 
\phivec{i}{n}{a[i] + \left( \phivec{j}{n}{2 \cdot b[j]} \right)[i]} \quad\leadsto\quad \philet{c}{\phivec{i}{n}{2 \cdot b[i]}}{\phivec{i}{n}{a[i] + c[i]}} 
$$
We consider both of these transformations to be a special cases of let insertion. As such, the implementation focuses on determining the expressions which should be let-bound. In addition, we need to determine the program point at which the binding should be performed. For simplicity, we associate program points with nodes in the term graph -- which are just terms. We have reduced the problem to determining for each term $t$, a list of terms $t'$ so that we transform $t \leadsto \philet{x}{t'}{[x/t']t}$. The substitution of a variable $x$ for $t'$ is performed by syntactic equivalence.

An important simplification of the problem setup is that Phi calculus is entirely pure (there are no side effects). Any let insertions performed in this manner of ``reverse-substitution'' preserve program semantics. Additionally, we make use of the assumption that all subexpressions are evaluated at least once. Recall that conditionals in Phi evaluate both conditional branches, and the premise is reasonable for loop constructs.

\paragraph{Common subexpression elimination} We perform CSE via a standard method on term graphs, which is by application of \textbf{dominators}. Consider a graph in which there is an edge $t \to t'$ whenever $t'$ is a direct subterm of $t$. Then consider the immediate dominator $t^*$ of a term $t$. By definition, we learn that $t^*$ is the smallest term that contains all instances of $t$ as a subterm. In fact, this indicates that we should let-bind $t$ at $t^*$. We perform this let-insertion for every non-atomic term $t$ for which $\mathrm{indeg}\,t > 1$. This common subexpression elimination produces an abstract syntax tree given any term graph.

We compute the immediate dominators with the \texttt{networkx} package, which is done in $O(n^2)$ worst-case time. An implementation for directed acyclic graphs could be optimised to $O(n \log n)$.

\paragraph{Loop-invariant code motion} We tackle the problem of recomputing values invariant across loop iterations. We want to compute these prior to the loop. We consider \textit{loop terms} to be $\Phi$ and $\mathrm{fold}$. 

Since after CSE we have an AST, we can perform a tree traversal on it. For each term $t$, there exist a stack of loop terms that it is a subterm of, and we can keep update it as part of the traversal. Each of these loop terms $s_\ell$ (and any let-bindings inside) introduces a set of symbols $S_\ell$, so the stack induces a sequence $(S_1, S_2, \dots, S_k)$. Denote the free symbols in $t$ as $T$. We make the following observation about scopes: if $\left( S_\ell \cup \cdots \cup S_k \right) \cap T = \emptyset$, then it is safe to let-bind $t$ prior to the loop $s_\ell$. On the other hand, $t$ will be computed the fewest times if it is moved to the outermost loop. Hence, we pick the minimum $\ell$ that preserves lexical scopes. We find and insert this let-binding for all non-atomic terms $t$.

A particularly tricky part of the implementation is the ordering in which these insertions are performed. An attentive reader may notice that the wrong choices result in scope errors or infinite terms. However, a careful implementation avoids these issues. 

Lastly, there are cases in which bindings may become trivial, i.e. of the form $\philet{x}{y}{e}$. To erase these, we perform a basic form of \textbf{copy propagation} by inlining (substituting) all trivial bindings.

\subsection{Array-of-structs to struct-of-arrays}

A standard transformation in array programming is the array of structs (AoS) to struct of arrays (SoA) representation transformation. Using it, the Ein compiler reduces all arrays to simple arrays of primitive types, as our compilation target -- NumPy,\footnote{Technically, NumPy does allow defining custom \texttt{dtype}s, but this feature is limited in use and intended for (de)serialising binary data. Corresponding features are not available in deep learning frameworks, so relying on this is undesirable.} and array libraries generally -- lack support for arrays of composite types like pairs. Furthermore, recall the definition of types in Phi. Once we eliminate arrays of pairs, the only types $\tau'$ are given by:
\begin{align*}
\alpha &::= \sigma \mid \Box \alpha \\
\tau' &::= \alpha \mid \tau' \times \tau'
\end{align*}
Hence, $\tau'$ can only be tuples of multidimensional arrays. This vastly simplifies compilation. 

The isomorphism the operation relies on is $\Box (\tau_1 \times \tau_2) \cong \Box \tau_1 \times \Box \tau_2$, i.e. for each array of pairs there exist an equivalent pair of arrays (with all arrays the same size). As such, we need only consider the introduction and elimination forms of arrays of pairs -- comprehensions and indexing. This is shown in Figure \ref{fig:aos-to-soa}, and we apply the transformation directly on the term graph. AoS-to-SoA transformations are also applied in Futhark and $\tilde F$. However, it is not described in detail for $\tilde F$ in the relevant paper.

\newcommand{\phisoa}[1]{\mathcal S \left\llbracket {#1} \right\rrbracket}
\newcommand{\phitupleindex}[2]{\mathcal P \left\llbracket {#1}, {#2} \right\rrbracket}

\begin{figure}[h]
    \centering
    \begin{align*}
\phisoa {\phivec{i}{e_n}{e}}
&= \phipair{\phivec{i'}{\phisoa{e_n}}{\phifst{\phisoa{e}}}}{\phivec{i''}{\phisoa{e_n}}{\phisnd{\phisoa{e}}}}
&\text{(where }\Gamma; \Delta, i \vdash e : \phipairtype{\tau_1}{\tau_2} \text{)} \\[0.5em]
\phitupleindex{e}{e'}
&= \phipair{\phitupleindex{\phifst{e}}{e'}}{\phitupleindex{\phisnd{e}}{e'}}
&\text{(where }\Gamma; \Delta \vdash e : \phipairtype{\tau_1}{\tau_2} \text{)} \\
\phitupleindex{e}{e'}
&= e\left[ e' \right]
&\text{(where }\Gamma; \Delta \vdash e : \phivectype{\tau} \text{)} \\
\phisoa{e[e']}
&= \phitupleindex{\phisoa{e}}{\phisoa{e'}}
&\text{(where }\Gamma; \Delta \vdash e : \phivectype{\tau} \text{)} \\[0.5em]
\phisoa{e + e'}
&= \phisoa{e} + \phisoa{e'}
&\text{(and similarly for other cases)} \\[0.5em]
\phisoa{\phivectype{(\phipairtype{\tau_1}{\tau_2}})} &= \phipairtype{\phivectype{\tau_1}}{\phivectype{\tau_2}} &\text{(identity otherwise)}
    \end{align*}
    \caption{AoS-to-SoA transformation on Phi, written $\mathcal S$. $\mathcal P$ maps an indexing to all arrays in the tuple. Environments $\Gamma; \Delta$ are passed through implicitly, in accordance with typing rules.}
    \label{fig:aos-to-soa}
\end{figure}

\section{Code generation -- Escaping the Pointless}

\subsection{Array calculus}

\textit{We first formalise our compilation target for Phi, since so far it has only been vaguely described as the API of the NumPy library.} \todothis

\subsection{Axials}

\textit{\textbf{Axials} are the key research contribution of this work.}

$$ \mathrm{VarEnv} \times \mathrm{IndexEnv} \to \mathrm{Value} \,\simeq\, \mathrm{VarEnv} \to \mathrm{NDArray}\, \mathrm{Value} $$

\todothis

% Compilation contract

\subsection{Optimisations}

\todothis

\section{Runtime -- Execution backends}

The compilation target of Ein is a high-level library in the array programming model. This means that most of the work is spent in relatively expensive whole-array operations, which dominates many overheads associated with an interpreter written in Python. As such, we can take liberties to simplify the implementation. The primary technique we use to form an execution backend is \textit{staging}. In essence, we implement a function of the form
$$ \mathrm{Expr} \to \left( \mathrm{Env} \to \mathrm{Value} \right) $$
where $\mathrm{Expr}$ is an expression in Phi or array calculus, and $\mathrm{Env}$ is a mapping from variables to assigned values. This essentially corresponds to directly implementing the denotational semantics $\llbracket - \rrbracket : \mathrm{Env} \to \mathrm{Value}$ of the two calculi, where we implement the denotations as Python closures.

\subsection{Naive}

The first execution backend implemented was the \textit{naive Python interpreter}. It directly implements the denotational semantics of Phi, so Axial compilation is not necessary. We use Python's builtin types to represent Phi values. Thanks to the dynamically typed nature of these types, we representing arrays of pairs does not necessitate the AoS-to-SoA transformation. However, we still have to apply common subexpression elimination and loop-invariant code motion to avoid combinatorial explosion of the time complexity.

This backend was primarily used as a reference as time went on, as extending it was easiest. It focused on correctness rather than any notion of performance, and it is order of magnitudes slower than the array library backends. To give an example of the naive backend's behaviour, staging $\phivec{i}{n}{2 \cdot i}$ results in a Python function \mintinline{python}{(dict[Variable, Value]) -> Value} semantically equal to
\begin{center}
\begin{cminted}{python}
lambda env: [2 * i for i in range(env[n])]
\end{cminted}    
\end{center}

\subsection{NumPy}

We follow the same denotation-guided staging approach as in the naive backend. This time around we apply the whole suite of transformations and use Axial compilation from Phi to the array calculus. The key difference is a stricter value representation that must rely on NumPy's \texttt{ndarray}s. We set all values to be either arrays, or a tuple of arrays. On this basis, we also apply some NumPy-specific performance optimisations.

\paragraph{Bindings} In some cases, NumPy attempts not to copy values where possible, and rather modifies their stride representation. For instance, a matrix might be transposed with \texttt{numpy.transpose(a, (1, 0))}. However, NumPy merely marks the memory layout as transposed -- it returns a \textit{view} -- rather than returning a transposed copy of the data. This is beneficial when the value is used just once, but harmful when it is reused. We make use of a heuristic approach to ensure let-bound values have a cache-efficient memory layout. Whenever a view would be bound to a variable, it is immediately copied to the target layout instead. 

It is worth noting here that Ein's let-bindings allow more efficient memory management than a manually-written NumPy program. Usually, new arrays are allocated sequentially with no explicit deallocations. Memory can only be freed by the garbage collector at the end of a function's execution. With let bindings, we achieve higher granularity -- arrays can be freed as soon as they are no longer used.

\paragraph{Eliminating temporaries} Where they cannot return a view, NumPy operations allocate fresh memory for the result by default. Operations can be made in-place instead, but idiomatic NumPy programs are difficult to write in this style. 
This state of matters is suboptimal. Where we want to add three vectors \texttt{(a, b, c)}, the computation \texttt{a + b + c} will allocate memory for both the temporary \texttt{a + b} and the result of \texttt{(a + b) + c}. If we know the former of these will not be reused (which is easy to determine in an AST), we can reuse its memory. In practice, one would write:
\begin{center}
\begin{cminted}{python}
t = numpy.add(a, b)            # Store t = a + b
return numpy.add(t, c, out=t)  # Reuse t for a + b + c
\end{cminted}
\end{center}
Ein's NumPy backend performs this in-place update optimisation automatically, which is more cache-friendly and saves needless memory allocations. It uses a heuristic approach based on the above observation, and including a check that the destination (\texttt{out}) has the appropriate shape and type.

\subsection{Generalising the NumPy backend}

\paragraph{Context \& Motivation} The very same methods as in the NumPy can be used in other array programming frameworks -- such as PyTorch, TensorFlow, Jax. This is because they all derive from NumPy, as explained in the Preparation. As a project extension, a PyTorch backend was implemented. PyTorch is a deep learning framework that allows efficient execution on GPUs and other hardware accelerators. It features a set of well-optimised \textit{kernels} for operations common in the domain, with memory movement and asynchronous device calls handled transparently.

This project does not explore use of accelerators in much detail. However, it is greatly important to stress the simplicity with which a backend relying on a CPU-oriented NumPy interface is generalised. Existing frameworks already implement hundreds of thousands of lines of code to take advantage of accelerators. This can have orders of magnitude differences in performance in practice.

\paragraph{Automatic differentation} A feature common to all deep learning frameworks is the capability of performing \textit{automatic differentiation} (AD). Notably, we do not implement it directly in Ein. This choice is motivated by AD's availability in Ein's potential backends. Indeed, to differentiate an Ein function, we may use the PyTorch backend and make use of its existing facilities. In the following we compute $\frac{\partial \mathbf{a}^T \mathbf{b}}{\partial \mathbf{a}} = \mathbf{b}$:
\begin{center}
\begin{cminted}{python}
import torch
# Initialise a pair of vectors with gradient tracking
a, b = (torch.randn(3).requires_grad_(True) for _ in range(2))
# Compute a dot product in Ein and execute in PyTorch
c = reduce_sum(lambda i: wrap(a)[i] * wrap(b)[i]).torch()
# Run backpropagation in torch through Ein's torch graph
c.backward()
return a.grad
\end{cminted}
\end{center}

\paragraph{Abstract backend} To facilitate addition of new backends, the \texttt{AbstractArrayBackend} base class was implemented. A staging function may be derived from an instance of this interface. Both the NumPy and PyTorch backends actually implement this same interface. Nearly no extra glue code is necessary, as the only work is necessary is implementing the translating Ein's array calculus to the right API calls.

\subsection{Extrinsics}

\todothis

\section{Repository overview}

The repository follows a usual Python project structure, with the root directory containing the \texttt{ein/} source directory, \texttt{tests/}, \texttt{tools/}, the \texttt{pyproject.toml} project configuration, a \texttt{README.md}, and Git repository files. The main source directory is split into three subpackages that correspond to each part of the compiler -- \texttt{frontend}, \texttt{midend}, and \texttt{backend}. The root package implements modules common to all of these, particularly the definition of the Phi calculus and various facilities surrounding it. The tests are split by the features they mainly apply, with the \texttt{suite/} subdirectory implementing larger example programs that were used for benchmarking. These benchmarks are run with \texttt{tools/benchmark.py}, which contains preconfigured benchmark runs for the suite, collecting and visualising the data.

% TODO/TOUSE: Where possible, defensive programming was applied to prevent compiler transformations returning code that invalidates expected properties. Most of the code is statically typed with annotations, and it was regularly checked with mypy.