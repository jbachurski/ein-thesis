\chapter{Example benchmark code}

In this appendix, we consider the Python implementation of the GAT benchmark case -- both using Ein and NumPy. The case is based on an open-source implementation (in \href{https://github.com/google-deepmind/clrs/blob/8697f51663bd77548f4b3108816c84d163883361/clrs/_src/processors.py#L99}{\texttt{deepmind/clrs}} on GitHub) of Graph Attention Networks which used JAX, and which I translated by hand.

The excerpts from the implementations omit the surrounding test harness -- all free variables should be assumed to be function arguments, and imports (such as \mintinline{python}{from ein import array}) are presumed.

\section*{Ein}

\begin{minted}{python}
def softmax(x: Vec[T]) -> Vec[T]:
    x_max = fold_max(lambda i: x[i])
    x1 = array(lambda i: (x[i] - x_max).exp())
    x1_sum = fold_sum(lambda i: x1[i])
    return array(lambda i: x1[i] / x1_sum)
def leaky_relu(x: Scalar) -> Scalar:
    return where(x < 0.0, 0.01 * x, x)
bias = array(lambda b, u, v: (adj[b, u, v] - 1.0) * 1e9)
logits = array(lambda b, h, u, v: s[b, u, h] + t[b, v, h] + e[b, u, v, h] + g[b, h])
coefs = array(
    lambda b, h, u: softmax(array(lambda v: leaky_relu(logits[b, h, u, v]) + bias[b, u, v]))
)
return array(lambda b, u, h, f: fold_sum(lambda v: coefs[b, h, u, v] * vals[b, v, h, f]))
\end{minted}

\section*{NumPy}

\begin{minted}{python}
batches, vertices, heads = s.shape
bias = (adj - 1.0) * 1e9
bias = numpy.tile(bias[..., None], (1, 1, 1, heads))  # [B, N, N, H]
bias = numpy.transpose(bias, (0, 3, 1, 2))  # [B, H, N, N]
vals = numpy.transpose(vals, (0, 2, 1, 3))  # [B, H, N, F]
logits = (
    numpy.transpose(s[..., numpy.newaxis], (0, 2, 1, 3))  # + [B, H, N, 1]
    + numpy.transpose(t[..., numpy.newaxis], (0, 2, 3, 1))  # + [B, H, 1, N]
    + numpy.transpose(e, (0, 3, 1, 2))  # + [B, H, N, N]
    + g[..., numpy.newaxis, numpy.newaxis]  # + [B, H, 1, 1]
)  # = [B, H, N, N]
coefs = scipy.special.softmax(leaky_relu(logits) + bias, axis=-1)
ret = numpy.matmul(coefs, vals)  # [B, H, N, F]
return numpy.transpose(ret, (0, 2, 1, 3))  # [B, N, H, F]
\end{minted}

\chapter{Benchmark results}

This appendix provides extended benchmark results generated across many problem sizes, depicting any evident uncertainties and overheads associated with Ein. This solidifies arguments that Ein's performance is consistently at an satisfactory (though not perfectly optimal) level. 

This data further includes performance measurements for Ein's alternative backends -- PyTorch and JAX -- on cases based on deep learning domain programs. We did not consider this when evaluating Ein itself, as these backends have characteristics entirely independent from NumPy, hence comparisons to NumPy baselines would be unfair. Since JAX applies ahead-of-time compilation, it performs best of all.


\pgfplotsset{width=0.46\textwidth}
\begin{center}
\input{Sources/benchmark-plots/Attention}
\input{Sources/benchmark-plots/GAT}
\end{center}
\begin{center}
\input{Sources/benchmark-plots/Fun with Semirings}
\input{Sources/benchmark-plots/MRI-Q}
\end{center}
\begin{center}
\input{Sources/benchmark-plots/Stencil}
\input{Sources/benchmark-plots/Hotspot}
\end{center}
\begin{center}
\input{Sources/benchmark-plots/Pathfinder}
\input{Sources/benchmark-plots/NN}
\end{center}

\chapter{Compiler outputs}

This brief appendix concerns the compiler outputs for a short example program. Intermediate representations in Phi and Yarr are printed using the compiler's debug utilities. For clarity, a hand-written Python program using NumPy which is equivalent to the Yarr code is given.

\section*{Ein}

We use Ein's Python API to implement \textcolor{blue}{\texttt{pairwiseL1}}. Type annotations are applied, and the code is successfully type-checked by \texttt{mypy}.

\begin{center}    
\begin{cminted}{python}
from ein import array, fold, Vec, Int, Float
from typing import Callable

def fold_sum(f: Callable[[Int], Float]) -> Float:
    return fold(wrap(0.0), lambda i, acc: acc + f(i))

def L1(u: Vec[Float], v: Vec[Float]) -> Float:
    return fold_sum(lambda i: abs(u[i] - v[i]))

def pairwiseL1(A: Vec[Vec[Float]]) -> Vec[Vec[Float]]:
    return array(lambda i, j: L1(A[i], A[j]))
\end{cminted}
\end{center}

\section*{Phi}

We construct the Phi term for \texttt{pairwiseL1} via 
\mintinline{python}{ein.function(pairwiseL1).phi(ein.ndarray_type(2, float))}
The term is printed as follows (\texttt{\&x} denote variables, where \texttt{\&0} is the argument \texttt{A}, \texttt{@i} are indices):
\begin{center}
\begin{cminted}{ocaml}
let &1: int = size[0](&0) in
let &2: int = size[1](&0) in
for @0[&1]. 
  for @1[&1]. 
    fold &3[&2] init &4: float = 0.0 by &4 => 
      &4 + (let &5: float = &0[@0][&3] - &0[@1][&3] in
            if less(0.0, &5) then &5 else -&5)
\end{cminted}
\end{center}
To make the example more interesting, the absolute value is computed here via a conditional: 
$$|x| = \mathrm{where}(0 < x, x, -x)$$

\section*{Yarr}

We apply code generation on Phi to obtain Yarr code, which uses primitives similar to NumPy routines.

\begin{center}    
\begin{cminted}{ocaml}
let &6: int = size[1](&0) in
let &7: [][]float = unsqueeze((0, 1), 0.0) in
let &8: [][]float = 
  let &9: int = size[0](&0) in
  repeat(1, &9, repeat(0, &9, &7))
in
fold &3[&6] init &4: [][]float = &8 by &4 => 
  add(&4, 
    let &10: [][]float = 
      let &11: []float = take(&0, None, &3) in
      subtract(unsqueeze((1,), &11), &11)
    in
    where(less(0.0, &10), &10, negative(&10))
  )
\end{cminted}
\end{center}

\section*{NumPy}

I hand-simplified the Yarr output into an equivalent Python program using NumPy directly. 

\begin{center}    
\begin{cminted}{python}
from numpy import *

def pairwiseL1(x0: ndarray) -> ndarray:
    x6, x9 = x0.shape[1]
    x4 = zeros((x6, x9))
    for i in range(x1):
        x11 = x0[:, i]
        x10 = expand_dims(x11, axis=1) - x11
        x4 += where(x10 < 0, x10, -x10)
    return x4
\end{cminted}
\end{center}
