@article{Paszke2021GettingTT,
  title={Getting to the point: index sets and parallelism-preserving autodiff for pointful array programming},
  author={Adam Paszke and Daniel D. Johnson and David Kristjanson Duvenaud and Dimitrios Vytiniotis and Alexey Radul and Matthew J. Johnson and Jonathan Ragan-Kelley and Dougal Maclaurin},
  journal={Proceedings of the ACM on Programming Languages},
  year={2021},
  volume={5},
  pages={1 - 29},
  url={https://api.semanticscholar.org/CorpusID:233210711}
}
@book{10.5555/1098666,
author = {Iverson, Kenneth E.},
title = {A Programming Language},
year = {1962},
isbn = {0471430145},
publisher = {John Wiley \& Sons, Inc.},
address = {USA}
}
@article {maclaurin2019dex,
	title = {Dex: array programming with typed indices},
	journal = {NeurIPS Program Transformations},
	year = {2019},
	author = {Dougal Maclaurin and Radul, Alexey and Johnson, Matthew J and Vytiniotis, Dimitrios}
}
@misc{Rush_2018, url={https://nlp.seas.harvard.edu/NamedTensor}, title={Tensor Considered Harmful}, author={Rush, Alexander}, year={2018}} 
@article{10.14778/3407790.3407799,
author = {Wang, Yisu Remy and Hutchison, Shana and Leang, Jonathan and Howe, Bill and Suciu, Dan},
title = {{SPORES}: Sum-Product Optimization via Relational Equality Saturation for Large Scale Linear Algebra},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407799},
doi = {10.14778/3407790.3407799},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1919–1932},
numpages = {14}
}
@inproceedings{Henriksen:2017:FPF:3062341.3062354,
 author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
 title = {{Futhark}: Purely Functional {GPU}-programming with Nested Parallelism and In-place Array Updates},
 booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2017},
 year = {2017},
 isbn = {978-1-4503-4988-8},
 location = {Barcelona, Spain},
 pages = {556--571},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3062341.3062354},
 doi = {10.1145/3062341.3062354},
 acmid = {3062354},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, compilers, functional language, parallel},
}
@article{Smith2018opt_einsum,
author = {Smith, Daniel and Gray, Johnnie},
year = {2018},
month = {06},
pages = {753},
title = {opt\_einsum - A Python package for optimizing contraction order for einsum-like expressions},
volume = {3},
journal = {Journal of Open Source Software},
doi = {10.21105/joss.00753}
}
@article{abbott2022tullio, title={mcabbott/Tullio.jl: v0.3.5}, DOI={10.5281/zenodo.7106192}, abstractNote={Tullio v0.3.5 <p><a href="https://github.com/mcabbott/Tullio.jl/compare/v0.3.4.v0.3.5">Diff since v0.3.4</a></p> <p><strong>Closed issues:</strong></p> <ul> <li>Using Tullio within generated functions (#149)</li> <li>poor performance for simple GPU loop (#152)</li> <li>Symbolic gradient producing surprising results (#153)</li> </ul> <p><strong>Merged pull requests:</strong></p> <ul> <li>Don\'t call <code>parent</code> before <code>similar</code> (#159) (@mcabbott)</li> </ul>}, publisher={Zenodo}, author={Michael Abbott and Dilum Aluthge and N3N5 and Simeon Schaub and Chris Elrod and Carlo Lucibello and Johnny Chen}, year={2022}, month={Sep} }
@article{numpy,
  author={van der Walt, Stefan and Colbert, S. Chris and Varoquaux, Gael},
  journal={Computing in Science \& Engineering}, 
  title={The {NumPy} Array: A Structure for Efficient Numerical Computation}, 
  year={2011},
  volume={13},
  number={2},
  pages={22-30},
  doi={10.1109/MCSE.2011.37}
}
@inproceedings{
    rogozhnikov2022einops,
    title={Einops: Clear and Reliable Tensor Manipulations with {Einstein}-like Notation},
    author={Alex Rogozhnikov},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=oapKSVM2bcj}
}
@inproceedings{10.1145/3460944.3464310,
author = {Henriksen, Troels and Elsman, Martin},
title = {Towards Size-Dependent Types for Array Programming},
year = {2021},
isbn = {9781450384667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460944.3464310},
doi = {10.1145/3460944.3464310},
abstract = {We present a type system for expressing size constraints on array types in an ML-style type system. The goal is to detect shape mismatches at compile-time, while being simpler than full dependent types. The main restrictions is that the only terms that can occur in types are array sizes, and syntactically they must be variables or constants. For those programs where this is not sufficient, we support a form of existential types, with the type system automatically managing the requisite book-keeping. We formalise a large subset of the type system in a small core language, which we prove sound. We also present an integration of the type system in the high-performance parallel functional language Futhark, and show on a collection of 44 representative programs that the restrictions in the type system are not too problematic in practice.},
booktitle = {Proceedings of the 7th ACM SIGPLAN International Workshop on Libraries, Languages and Compilers for Array Programming},
pages = {1-14},
numpages = {14},
keywords = {type systems, parallel programming, functional programming},
location = {Virtual, Canada},
series = {ARRAY 2021}
}
@article{tensor-comprehensions,
author = {Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
year = {2018},
month = {02},
pages = {},
title = {Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions}
}
@inproceedings{kjolstad:2017:tools, 
  author={Kjolstad, Fredrik and Chou, Stephen and Lugato, David and Kamil, Shoaib and Amarasinghe, Saman}, 
  booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={taco: A Tool to Generate Tensor Algebra Kernels}, 
  year={2017}, 
  pages={943-948}, 
  keywords={data analysis;learning (artificial intelligence);mathematics computing;program compilers;software libraries;tensors;code generation tool;code generator;command-line tools;compressed formats;computational abstraction;data analytics;dense kernels;machine learning;mixed kernels;physical sciences;sparse kernels;taco web;tensor algebra expressions;tensor algebra kernels;tensor expressions;Indexes;Kernel;Libraries;Linear algebra;Tensile stress;Tools;Tensor algebra;compiler;linear algebra;sparse}, 
  doi={10.1109/ASE.2017.8115709}, 
  month={Oct}
}
@inproceedings{maziarz2021hashing,
author = {Maziarz, Krzysztof and Ellis, Tom and Lawrence, Alan and Fitzgibbon, Andrew and Peyton Jones, Simon},
title = {Hashing Modulo Alpha-Equivalence},
organization = {ACM},
booktitle = {ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'21)},
year = {2021},
month = {June},
abstract = {In many applications one wants to identify identical subtrees of a program syntax tree.  This identification should ideally be robust to alpha-renaming of the program, but no existing technique has been shown to achieve this with good efficiency (better than O(n^2) in expression size). We present a new, asymptotically efficient way to hash modulo alpha-equivalence. A key insight of our method is to use a weak (commutative) hash combiner at exactly one point in the construction, which admits an algorithm with O(n*(log n)^2) time complexity. We prove that the use of the commutative combiner nevertheless yields a strong hash with low collision probability.},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/hashing-modulo-alpha-equivalence-2/},
}
@inproceedings {9835658,
author = {A. Sankaran and N. Alashti and C. Psarras and P. Bientinesi},
booktitle = {2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
title = {Benchmarking the Linear Algebra Awareness of {TensorFlow} and {PyTorch}},
year = {2022},
volume = {},
issn = {},
pages = {924-933},
abstract = {Linear algebra operations, which are ubiquitous in machine learning, form major performance bottlenecks. The High-Performance Computing community invests significant effort in the development of architecture-specific optimized kernels, such as those provided by the BLAS and LAPACK libraries, to speed up linear algebra operations. However, end users are progressively less likely to go through the error prone and time-consuming process of directly using said kernels; instead, frameworks such as TensorFlow (TF) and PyTorch (PyT), which facilitate the development of machine learning applications, are becoming more and more popular. Although such frameworks link to BLAS and LAPACK, it is not clear whether or not they make use of linear algebra knowledge to speed up computations. For this reason, in this paper we develop benchmarks to investigate the linear algebra optimization capabilities of TF and PyT. Our analyses reveal that a number of linear algebra optimizations are still missing; for instance, reducing the number of scalar operations by applying the distributive law, and automatically identifying the optimal parenthesization of a matrix chain. In this work, we focus on linear algebra computations in TF and PyT; we both expose opportunities for performance enhancement to the benefit of the developers of the frameworks and provide end users with guidelines on how to achieve performance gains.},
keywords = {linear systems;linear algebra;machine learning;benchmark testing;performance gain;parallel processing;matrices},
doi = {10.1109/IPDPSW55747.2022.00150},
url = {https://doi.ieeecomputersociety.org/10.1109/IPDPSW55747.2022.00150},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}
@misc{Liu_2020, title={A type system for multidimensional arrays}, url={https://dash.harvard.edu/handle/1/37364746}, journal={DASH Home}, author={Liu, Theodore Tiger}, year={2020}} 
@inproceedings{Abe2015ASA,
  title={A Simple and Practical Linear Algebra Library Interface with Static Size Checking},
  author={Akinori Abe and Eijiro Sumii},
  booktitle={ML/OCaml},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:5608693}
}
@inproceedings{10.1145/2628136.2628138,
author = {Gibbons, Jeremy and Wu, Nicolas},
title = {Folding Domain-Specific Languages: Deep and Shallow Embeddings (Functional Pearl)},
year = {2014},
isbn = {9781450328739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628136.2628138},
doi = {10.1145/2628136.2628138},
abstract = {A domain-specific language can be implemented by embedding within a general-purpose host language. This embedding may be deep or shallow, depending on whether terms in the language construct syntactic or semantic representations. The deep and shallow styles are closely related, and intimately connected to folds; in this paper, we explore that connection.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
pages = {339–347},
numpages = {9},
keywords = {domain-specific languages, deep and shallow embedding, folds},
location = {Gothenburg, Sweden},
series = {ICFP '14}
}
@article{10.1145/2692915.2628138,
author = {Gibbons, Jeremy and Wu, Nicolas},
title = {Folding Domain-Specific Languages: Deep and Shallow Embeddings (Functional Pearl)},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/2692915.2628138},
doi = {10.1145/2692915.2628138},
abstract = {A domain-specific language can be implemented by embedding within a general-purpose host language. This embedding may be deep or shallow, depending on whether terms in the language construct syntactic or semantic representations. The deep and shallow styles are closely related, and intimately connected to folds; in this paper, we explore that connection.},
journal = {SIGPLAN Not.},
month = {aug},
pages = {339–347},
numpages = {9},
keywords = {folds, domain-specific languages, deep and shallow embedding}
}
@inproceedings{jax2018,
  title={Compiling machine learning programs via high-level tracing},
  author={Frostig, Roy and Johnson, Matthew James and Leary, Chris},
  journal={Systems for Machine Learning},
  volume={4},
  number={9},
  year={2018},
  publisher={SysML}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}
@misc{50530,
title	= {{XLA} : Compiling Machine Learning for Peak Performance},
author	= {Amit Sabne},
year	= {2020}
}
@inproceedings{10.5555/3291168.3291211,
author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
year = {2018},
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms - such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
pages = {579–594},
numpages = {16},
location = {Carlsbad, CA, USA},
series = {OSDI'18}
}
@software{PyO3_Project_and_Contributors_PyO3,
author = {{PyO3 Project and Contributors}},
license = {["Apache-2.0", "MIT"]},
title = {{PyO3}}
}
@article{2021-egg,
    author = {Willsey, Max and Nandi, Chandrakana and Wang, Yisu Remy and Flatt, Oliver and Tatlock, Zachary and Panchekha, Pavel},
    title = {egg: Fast and Extensible Equality Saturation},
    year = {2021},
    issue_date = {January 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {5},
    number = {POPL},
    url = {https://doi.org/10.1145/3434304},
    doi = {10.1145/3434304},
    abstract = {An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites.  This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called e-class analyses integrates domain-specific analyses into the e-graph, reducing the need for ad hoc manipulation.  We implemented these techniques in a new open-source library called egg. Our case studies on three previously published applications of equality saturation highlight how egg's performance and flexibility enable state-of-the-art results across diverse domains.},
    journal = {Proc. ACM Program. Lang.},
    month = jan,
    articleno = {23},
    numpages = {29},
    keywords = {equality saturation, e-graphs}
  }
@inproceedings{2008-z3,
author = {de Moura, Leonardo and Bjørner, Nikolaj},
year = {2008},
month = {04},
pages = {337-340},
title = {{Z3}: an efficient {SMT} solver},
volume = {4963},
isbn = {978-3-540-78799-0},
journal = {Tools and Algorithms for the Construction and Analysis of Systems},
doi = {10.1007/978-3-540-78800-3_24}
}
@inproceedings{Stratton2012ParboilAR,
  title={Parboil: A Revised Benchmark Suite for Scientific and Commercial Throughput Computing},
  author={John A. Stratton and Christopher I. Rodrigues and I-Jui Sung and Nady Obeid and Li-Wen Chang and Nasser Anssari and Geng Liu and Wen-mei W. Hwu},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:497928}
}